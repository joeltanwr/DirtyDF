{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = pd.read_csv('../data/online_retail_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viknesh/NUS/artificalData/artificial-dataset-generation/ddf\n"
     ]
    }
   ],
   "source": [
    "cd ../ddf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddf.stainer import Stainer, RowDuplicateStainer, ColumnSplitter,ResidualResampler\n",
    "from ddf.DirtyDF import DirtyDF\n",
    "from numpy.random import default_rng\n",
    "from time import time\n",
    "rng= default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = oo.sample(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>536386</td>\n",
       "      <td>85099B</td>\n",
       "      <td>JUMBO BAG RED RETROSPOT</td>\n",
       "      <td>100</td>\n",
       "      <td>1/12/2010 9:57</td>\n",
       "      <td>1.65</td>\n",
       "      <td>16029.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>536412</td>\n",
       "      <td>22077</td>\n",
       "      <td>6 RIBBONS RUSTIC CHARM</td>\n",
       "      <td>1</td>\n",
       "      <td>1/12/2010 11:49</td>\n",
       "      <td>1.65</td>\n",
       "      <td>17920.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>536592</td>\n",
       "      <td>22570</td>\n",
       "      <td>FELTCRAFT CUSHION RABBIT</td>\n",
       "      <td>1</td>\n",
       "      <td>1/12/2010 17:06</td>\n",
       "      <td>7.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>536597</td>\n",
       "      <td>21221</td>\n",
       "      <td>SET/4 BADGES CUTE CREATURES</td>\n",
       "      <td>5</td>\n",
       "      <td>1/12/2010 17:35</td>\n",
       "      <td>1.25</td>\n",
       "      <td>18011.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     InvoiceNo StockCode                  Description  Quantity  \\\n",
       "177     536386    85099B      JUMBO BAG RED RETROSPOT       100   \n",
       "562     536412     22077       6 RIBBONS RUSTIC CHARM         1   \n",
       "2755    536592     22570     FELTCRAFT CUSHION RABBIT         1   \n",
       "3106    536597     21221  SET/4 BADGES CUTE CREATURES         5   \n",
       "\n",
       "          InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "177    1/12/2010 9:57       1.65     16029.0  United Kingdom  \n",
       "562   1/12/2010 11:49       1.65     17920.0  United Kingdom  \n",
       "2755  1/12/2010 17:06       7.62         NaN  United Kingdom  \n",
       "3106  1/12/2010 17:35       1.25     18011.0  United Kingdom  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = oo.sample(n=4)\n",
    "d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSplitter(Stainer):\n",
    "    \"\"\" Stainer to split text columns, creating a ragged DataFrame \"\"\"\n",
    "    \n",
    "    def __init__(self, name = \"Column splitter\", col_idx = [], regex_string=\" \"):\n",
    "        \"\"\" Constructor for ColumnSplitter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str, optional.\n",
    "            Name of stainer. \n",
    "        col_idx: int list, required. This has to be a single integer. This is \n",
    "                 the column that will be split into two.\n",
    "        regex_string: The string to split the column on. For instance, \"(?=-)\" is a\n",
    "                 look-ahead assertion that splits on a hyphen. Using lookahead or \n",
    "                 look-behind strings ensures that the splitting character is retained.\n",
    "                 \n",
    "                 \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If col_idx is missing, or is length greater than 1.\n",
    "        \"\"\"\n",
    "        if ((type(col_idx) is not list) or (len(col_idx) != 1)):\n",
    "            raise ValueError(\"col_idx must be a list with a single integer.\")\n",
    "        super().__init__(name, [], col_idx)\n",
    "        self.regex_string = regex_string\n",
    "        \n",
    "    def transform(self, df, rng, row_idx=None, col_idx=None):\n",
    "        \"\"\"Applies staining on the given indices in the provided dataframe.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame \n",
    "            Dataframe to be transformed.\n",
    "        rng : np.random.BitGenerator\n",
    "            PCG64 pseudo-random number generator. Unused by this stainer.\n",
    "        row_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        col_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_df : pd.DataFrame\n",
    "            Modified dataframe, with one extra column on the right.\n",
    "        row_map : empty dictionary\n",
    "            Row mapping showing the relationship between the original and new row positions.\n",
    "        col_map : A dictionary\n",
    "            Generating the column map is tricky in this situation, because after the split column,\n",
    "            on the right, each column actually contains information from two original columns. One option \n",
    "            is to indicate which two columns each new column maps to, but, since most of the original \n",
    "            columns were retained, we only indicate that the right-most column has been mapped to the\n",
    "            last two of the new dataframe.\n",
    "       \n",
    "        >>> rng = np.random.default_rng(12)\n",
    "        >>> x = pd.DataFrame({'label': ['T-LIGHT', 'ASHTRAY'], 'price': [2.30, 3.20]})\n",
    "        >>> print(x)\n",
    "             label  price\n",
    "        0  T-LIGHT    2.3\n",
    "        1  ASHTRAY    3.2\n",
    "        \n",
    "        >>> cc = ColumnSplitter(col_idx=[0], regex_string=\"(?=-)\")\n",
    "        >>> new_x, rmap, cmap = cc.transform(x, rng)\n",
    "        >>> print(new_x) # see the ragged dataframe.. it contains unnamed column on the right.\n",
    "             label   price     \n",
    "        0        T  -LIGHT  2.3\n",
    "        1  ASHTRAY     3.2  NaN\n",
    "        \n",
    "        >>> print(cmap)\n",
    "        {0: [0], 1: [1, 2]}\n",
    "        \n",
    "        \"\"\"\n",
    "        new_df, row_idx, col_idx = self._init_transform(df, row_idx, col_idx)\n",
    "        start = time()\n",
    "        \n",
    "        org_col_index = new_df.columns\n",
    "        col_name = org_col_index[col_idx[0]]\n",
    "        split_id = np.argwhere(org_col_index == col_name).reshape(1)[0]\n",
    "        \n",
    "        # split the original df into three sections: \n",
    "        # to_keep, col_to_split and cols_to_join_back\n",
    "        to_keep = new_df.iloc[:, :split_id].copy(deep=True)\n",
    "        to_split = df[[col_name]].copy(deep=True)\n",
    "        to_join = df.iloc[:, (split_id+1):].copy(deep=True)\n",
    "        cols_to_add = np.hstack((org_col_index[(split_id+1):], ''))\n",
    "        to_join[''] = pd.Series([np.NaN]*to_join.shape[0])\n",
    "        \n",
    "        #split the column:\n",
    "        to_split = to_split[col_name].str.split(self.regex_string, n=1, expand=True)\n",
    "        to_split.columns = [col_name, cols_to_add[0]]\n",
    "        \n",
    "        # join the split column back first\n",
    "        #to_keep = pd.concat([to_keep, to_split], axis=1)\n",
    "        to_keep = to_keep.join(to_split)\n",
    "        na_boolean = to_keep[cols_to_add[0]].isna()\n",
    "        to_keep = to_keep.combine_first(to_join[[cols_to_add[0]]])\n",
    "        #breakpoint()\n",
    "        \n",
    "        for i in np.arange(1,len(cols_to_add)):\n",
    "            #print(i)\n",
    "            new_col = to_join.iloc[:, i].copy(deep=True)\n",
    "            new_col[~na_boolean] = to_join.iloc[:, i-1][~na_boolean]\n",
    "            to_keep = pd.concat([to_keep, new_col], axis=1)\n",
    "            #print(cols_to_add[i])\n",
    "        #print(cols_to_add)\n",
    "        #return to_keep[np.hstack((org_col_index, ''))]\n",
    "        \n",
    "        # create col-map:\n",
    "        cmap = {}\n",
    "        for ii in np.arange(df.shape[1]):\n",
    "            cmap[ii] = [ii]\n",
    "        cmap[df.shape[1]-1] = [df.shape[1]-1, df.shape[1]]\n",
    "        # print(cmap)\n",
    "        \n",
    "        end = time()\n",
    "        self.update_history(f\"Column id {col_name} split into two.\", end-start)\n",
    "        self.update_history(message = f\"New dataframe has {to_keep.shape[1]} columns now.\")\n",
    "        return to_keep[np.hstack((org_col_index, ''))],{},cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(ColumnSplitter.transform, globs=None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "x = pd.DataFrame({'label': ['T-LIGHT', 'ASHTRAY'], 'price': [2.30, 3.20]})\n",
    "cc = ColumnSplitter(col_idx=[0], regex_string=\"(?=-)\")\n",
    "new_x, rmap, cmap = cc.transform(x, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     InvoiceNo StockCode               Description                  Quantity  \\\n",
       " 177     536386    85099B   JUMBO BAG RED RETROSPOT                       100   \n",
       " 562     536412     22077    6 RIBBONS RUSTIC CHARM                         1   \n",
       " 2755    536592     22570  FELTCRAFT CUSHION RABBIT                         1   \n",
       " 3106    536597     21221                       SET  /4 BADGES CUTE CREATURES   \n",
       " \n",
       "           InvoiceDate        UnitPrice  CustomerID         Country  \\\n",
       " 177    1/12/2010 9:57             1.65    16029.00  United Kingdom   \n",
       " 562   1/12/2010 11:49             1.65    17920.00  United Kingdom   \n",
       " 2755  1/12/2010 17:06             7.62         NaN  United Kingdom   \n",
       " 3106              5.0  1/12/2010 17:35        1.25         18011.0   \n",
       " \n",
       "                       \n",
       " 177              NaN  \n",
       " 562              NaN  \n",
       " 2755             NaN  \n",
       " 3106  United Kingdom  ,\n",
       " {},\n",
       " {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7, 8]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = ColumnSplitter(col_idx=[2], regex_string=\"(?=\\/)\")\n",
    "cc.transform(d2, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.index = np.arange(4) # this should be fixed, currently only works with 0..n-1 index\n",
    "                        # not true - original indexing can be used, but the output index will be 0...(n-1)\n",
    "#d2\n",
    "\n",
    "#rr= RowDuplicateStainer(deg=1/3, name='test dup row map', row_idx=[730, 3582])\n",
    "rr= RowDuplicateStainer(deg=1/3, name='test dup row map', row_idx=[0,1,2])\n",
    "\n",
    "#ddf1 = DirtyDF(d2, seed=1)\n",
    "#ddf1 = ddf1.add_stainers(rr)\n",
    "#ddf2 = ddf1.run_stainer()\n",
    "#ddf2.get_df()\n",
    "df,rmap,colmap = rr.transform(d2, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = {}\n",
    "#cmap.keys() = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in df.index:\n",
    "    cmap[ii] = [ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ColumnSplitter(col_idx=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = oo.sample(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original implementation\n",
    "\n",
    "def column_splitter(df, col_name, regex_string=\"(?=-)\"):\n",
    "    org_col_index = df.columns\n",
    "    split_id = np.argwhere(org_col_index == col_name).reshape(1)[0]\n",
    "    \n",
    "    # split the original df into three sections: \n",
    "    # to_keep, col_to_split and cols_to_join_back\n",
    "    to_keep = df.iloc[:, :split_id].copy(deep=True)\n",
    "    to_split = df[[col_name]].copy(deep=True)\n",
    "    to_join = df.iloc[:, (split_id+1):].copy(deep=True)\n",
    "    cols_to_add = np.hstack((org_col_index[(split_id+1):], ''))\n",
    "    to_join[''] = pd.Series([np.NaN]*to_join.shape[0])\n",
    "    \n",
    "    #split the column:\n",
    "    to_split = to_split[col_name].str.split(regex_string, n=1, expand=True)\n",
    "    to_split.columns = [col_name, cols_to_add[0]]\n",
    "    \n",
    "    # join the split column back first\n",
    "    #to_keep = pd.concat([to_keep, to_split], axis=1)\n",
    "    to_keep = to_keep.join(to_split)\n",
    "    na_boolean = to_keep[cols_to_add[0]].isna()\n",
    "    to_keep = to_keep.combine_first(to_join[[cols_to_add[0]]])\n",
    "    #breakpoint()\n",
    "    \n",
    "    for i in np.arange(1,len(cols_to_add)):\n",
    "        #print(i)\n",
    "        new_col = to_join.iloc[:, i].copy(deep=True)\n",
    "        new_col[~na_boolean] = to_join.iloc[:, i-1][~na_boolean]\n",
    "        to_keep = pd.concat([to_keep, new_col], axis=1)\n",
    "        #print(cols_to_add[i])\n",
    "    #print(cols_to_add)\n",
    "    \n",
    "    return to_keep[np.hstack((org_col_index, ''))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_splitter(d2, 'Description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco = pd.read_csv(\"../data/Telco-Customer-Churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_sub = telco.sample(n=10)\n",
    "telco_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = telco.copy(deep=True)\n",
    "\n",
    "d3 = d3.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnJoiner(Stainer):\n",
    "    \"\"\" Stainer to join text columns, creating a ragged DataFrame \"\"\"\n",
    "    \n",
    "    def __init__(self, name = \"Column splitter\", row_idx =[], col_idx = []):\n",
    "        \"\"\" Constructor for ColumnJoiner\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str, optional.\n",
    "            Name of stainer. \n",
    "        col_idx: int list, required. This has to contain two consecutive integers. These are \n",
    "                 the columns that will be catenated.\n",
    "        row_idx: int list, required. These will be the rows that will be \"shifted\" inwards.\n",
    "                 \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If col_idx has length not equals to two, or the values are not consecutive.\n",
    "        ValueError\n",
    "            If row_idx is empty.\n",
    "        \"\"\"\n",
    "        if ((len(col_idx) != 2) or (col_idx[1] - col_idx[0] != 1)):\n",
    "            raise ValueError(\"col_idx must contain two consecutive integers.\")\n",
    "        if(len(row_idx) == 0):\n",
    "            raise ValueError(\"row_idx should not be empty.\")\n",
    "        super().__init__(name, row_idx, col_idx)\n",
    "        \n",
    "    def transform(self, df, rng, row_idx=None, col_idx=None):\n",
    "        \"\"\"Applies staining on the given indices in the provided dataframe.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame \n",
    "            Dataframe to be transformed.\n",
    "        rng : np.random.BitGenerator\n",
    "            PCG64 pseudo-random number generator. Unused by this stainer.\n",
    "        row_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        col_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_df : pd.DataFrame\n",
    "            Modified dataframe, with some columns shifted \"inwards\"\n",
    "        row_map : empty dictionary.\n",
    "        col_map : empty dictionary.\n",
    "       \n",
    "        >>> rng = np.random.default_rng(12)\n",
    "        >>> x = pd.DataFrame({'label': ['T-LIGHT', 'ASHTRAY'], 'dept':['A1', 'A2'], 'price': [2.30, 3.20]})\n",
    "        >>> print(x)\n",
    "             label dept  price\n",
    "        0  T-LIGHT   A1    2.3\n",
    "        1  ASHTRAY   A2    3.2\n",
    "\n",
    "        >>> cj1 = ColumnJoiner('test joiner', [1], [0,1])\n",
    "        >>> print(cj1.transform(x, rng))\n",
    "               label dept  price\n",
    "        0    T-LIGHT   A1    2.3\n",
    "        1  ASHTRAYA2  3.2    NaN\n",
    "        \n",
    "        \"\"\"\n",
    "        new_df, row_idx, col_idx = self._init_transform(df, row_idx, col_idx)\n",
    "        start = time()\n",
    "                \n",
    "        org_col_index = new_df.columns\n",
    "        col_names = org_col_index[col_idx]\n",
    "        \n",
    "        split_id = np.argwhere(org_col_index == col_names[1]).reshape(1)[0]\n",
    "        to_keep = new_df.loc[:, :col_names[0]].copy(deep=True)\n",
    "        join_series = new_df[col_names[1]].copy(deep=True)\n",
    "        to_join = new_df.iloc[:, split_id:].copy(deep=True)\n",
    "\n",
    "        # breakpoint()\n",
    "        # modify the column to join, and paste with to_keep\n",
    "        join_series[~join_series.index.isin(row_idx)] = ''\n",
    "        to_keep[col_names[0]] = to_keep[col_names[0]] + join_series\n",
    "        \n",
    "        for i in np.arange(split_id, new_df.shape[1]):\n",
    "            # print(i)\n",
    "            new_col = new_df.iloc[:, i].copy(deep=True)\n",
    "            if i < (new_df.shape[1] - 1):\n",
    "                new_col[row_idx] = new_df.iloc[:, i+1][row_idx]\n",
    "            else:\n",
    "                new_col[row_idx] = np.NaN\n",
    "            to_keep = pd.concat([to_keep, new_col], axis=1)\n",
    "            #breakpoint()\n",
    "        \n",
    "        end = time()\n",
    "        self.update_history(f\"Column id {col_names[0]} and {col_names[1]} joined at certain rows.\", end-start)\n",
    "        return to_keep,{},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = ColumnJoiner('test', [1,4], [8, 9])\n",
    "dd.transform(d3, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctest.run_docstring_examples(dd.transform , globs=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = join_fn(telco_sub.MultipleLines, telco_sub.InternetService)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_fn(v1, v2):\n",
    "    return (v1 == 'No phone service') & (v2 == 'DSL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_joiner(df, col_name1, col_name2, join_criteria):\n",
    "    # join_index should be a boolean index that indicates which rows to join.\n",
    "    # use *row_idx* when we implement the stainer.\n",
    "    \n",
    "    org_col_index = df.columns\n",
    "    split_id = np.argwhere(org_col_index == col_name2).reshape(1)[0]\n",
    "    to_keep = df.loc[:, :col_name1].copy(deep=True)\n",
    "    join_series = df[col_name2].copy(deep=True)\n",
    "    to_join = df.iloc[:, split_id:].copy(deep=True)\n",
    "    \n",
    "    # breakpoint()\n",
    "    # modify the column to join, and paste with to_keep\n",
    "    join_series[~join_criteria] = ''\n",
    "    to_keep[col_name1] = to_keep[col_name1] + join_series\n",
    "    \n",
    "    for i in np.arange(split_id, df.shape[1]):\n",
    "        # print(i)\n",
    "        new_col = df.iloc[:, i].copy(deep=True)\n",
    "        if i < (df.shape[1] - 1):\n",
    "            new_col[join_criteria] = df.iloc[:, i+1][join_criteria]\n",
    "        else:\n",
    "            new_col[join_criteria] = np.NaN\n",
    "        to_keep = pd.concat([to_keep, new_col], axis=1)\n",
    "        #breakpoint()\n",
    "\n",
    "    return to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = column_joiner(telco_sub, 'MultipleLines', 'InternetService', c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_sub.loc[:, 'MultipleLines':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS, WLS\n",
    "from statsmodels.tools import add_constant\n",
    "from ddf.samplers import rand_from_Finv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualResampler(Stainer):\n",
    "    \"\"\" Stainer to resample residuals from a linear model and return new y's. \"\"\"\n",
    "    \n",
    "    def __init__(self, name = \"Residual resampler\", row_idx =[], col_idx = []):\n",
    "        \"\"\" Constructor for ResidualResampler\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str, optional.\n",
    "            Name of stainer. \n",
    "        col_idx: int list, required. This should specify at least two columns. The first will\n",
    "                 be used as the y-variable, and the others will be used as the X matrix.\n",
    "        row_idx: int list, unused.\n",
    "                 \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If col_idx has length not equals to two, or the values are not consecutive.\n",
    "        \"\"\"\n",
    "        if len(col_idx) < 2:\n",
    "            raise ValueError(\"col_idx must contain at least two integers.\")\n",
    "        super().__init__(name, row_idx, col_idx)\n",
    "        \n",
    "    def transform(self, df, rng, row_idx=None, col_idx=None):\n",
    "        \"\"\"Applies staining on the given indices in the provided dataframe.\n",
    "        \n",
    "        A ordinary least squares linear model is fit, using statsmodels. The residuals are then\n",
    "        sampled from using rand_from_Finv (from ddf.samplers) and added back to the fitted y-hats\n",
    "        to create a new set of y-values.\n",
    "        \n",
    "        This stainer should result in a similar fit, but slightly different diagnostics/statistics.\n",
    "        \n",
    "        The user should check if the new y-values are valid or not (e.g. are they negative when they \n",
    "        shouldn't be?)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame \n",
    "            Dataframe to be transformed.\n",
    "        rng : np.random.BitGenerator\n",
    "            PCG64 pseudo-random number generator.\n",
    "        row_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        col_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_df : pd.DataFrame\n",
    "            Modified dataframe, with some columns shifted \"inwards\"\n",
    "        row_map : empty dictionary.\n",
    "        col_map : empty dictionary.\n",
    "       \n",
    "        >>> rng = np.random.default_rng(12)\n",
    "        >>> x = np.arange(1, 11)\n",
    "        >>> y = x*2.3 + 3 + rng.normal(scale=1.6, size=10)\n",
    "        >>> org_df = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "        >>> rr = ResidualResampler('test rr', [], [1,0])\n",
    "        >>> new_df = rr.transform(org_df, rng)\n",
    "\n",
    "        >>> print(pd.concat((org_df, new_df), axis=1))\n",
    "            x          y   x          y\n",
    "        0   1   5.289077   1   4.155549\n",
    "        1   2   9.273829   2   8.747416\n",
    "        2   3  11.086541   3   8.444612\n",
    "        3   4  13.358330   4  11.250526\n",
    "        4   5  17.090042   5  14.005535\n",
    "        5   6  14.871107   6  16.610120\n",
    "        6   7  18.096871   7  18.940797\n",
    "        7   8  19.286939   8  23.466323\n",
    "        8   9  23.527596   9  21.974412\n",
    "        9  10  27.598022  10  23.545538\n",
    "\n",
    "        \"\"\"\n",
    "        new_df, row_idx, col_idx = self._init_transform(df, row_idx, col_idx)\n",
    "        start = time()\n",
    "        \n",
    "        # drop missing values\n",
    "        col_names = new_df.columns[col_idx]\n",
    "        fit_df = new_df.iloc[:, col_idx].dropna()\n",
    "        \n",
    "        X = add_constant(fit_df.iloc[:, 1:])\n",
    "        y = fit_df.iloc[:,0]\n",
    "    \n",
    "        # fit and predict from the model\n",
    "        m = OLS(y, X)\n",
    "        o = m.fit()\n",
    "        new_resid = rand_from_Finv(o.resid, rng, size=len(o.resid))\n",
    "        new_y = o.predict(X) + new_resid\n",
    "        \n",
    "        # only put new values where we could predict values; otherwise keep old y-values.\n",
    "        new_df.loc[y.index, col_names[0]] = new_y\n",
    "        \n",
    "        end = time()\n",
    "        self.update_history(f\"New y-values in column {col_idx[0]} by sampling from residual distribution.\", \n",
    "                            end-start)\n",
    "        return new_df,{},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = ResidualResampler('test rr', [], [5, 19])\n",
    "rr.transform(telco, rng).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "x = np.arange(1, 11)\n",
    "y = x*2.3 + 3 + rng.normal(scale=1.6, size=10)\n",
    "org_df = pd.DataFrame({'x':x, 'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = ResidualResampler('test rr', [], [1,0])\n",
    "new_df = rr.transform(org_df, rng)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    x          y\n",
       " 0   1   4.155549\n",
       " 1   2   8.747416\n",
       " 2   3   8.444612\n",
       " 3   4  11.250526\n",
       " 4   5  14.005535\n",
       " 5   6  16.610120\n",
       " 6   7  18.940797\n",
       " 7   8  23.466323\n",
       " 8   9  21.974412\n",
       " 9  10  23.545538,\n",
       " {},\n",
       " {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    x          y   x          y\n",
      "0   1   5.289077   1   6.891292\n",
      "1   2   9.273829   2  10.673205\n",
      "2   3  11.086541   3  12.731433\n",
      "3   4  13.358330   4  14.541629\n",
      "4   5  17.090042   5  12.720387\n",
      "5   6  14.871107   6  19.215727\n",
      "6   7  18.096871   7  19.722592\n",
      "7   8  19.286939   8  22.994630\n",
      "8   9  23.527596   9  22.492393\n",
      "9  10  27.598022  10  23.678673\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(pd.concat((org_df, new_df), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "org_df.plot(x='x', y='y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df.plot(x='x', y='y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco.tenure.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({'label': ['T-LIGHT', 'ASHTRAY'], 'dept':['A1', 'A2'], 'price': [2.30, 3.20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_from_Finv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import Akima1DInterpolator\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "def rand_from_Finv(X, size=(1,10), Xmin=None, Xmax = None):\n",
    "    rvs_needed = np.array(size).prod()\n",
    "    ecdf1 = ECDF(X)\n",
    "    U = np.hstack((0.0, ecdf1.y[1:-1], 1.0))\n",
    "    \n",
    "    if Xmin is None:\n",
    "        Xmin = X.min()\n",
    "    if Xmax is None:\n",
    "        Xmax = X.max()\n",
    "        \n",
    "    Finv = np.hstack((Xmin, ecdf1.x[1:-1], Xmax))\n",
    "    ak2 = Akima1DInterpolator(U, Finv)\n",
    "\n",
    "    U_rand = np.random.uniform(size=rvs_needed)\n",
    "    out = ak2(U_rand).reshape(size)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_residuals(df, ycol, Xcols):\n",
    "    breakpoint()\n",
    "    new_df = df.dropna()\n",
    "    X = add_constant(new_df[[Xcols]])\n",
    "    y = new_df[ycol]\n",
    "    \n",
    "    #drop_these = pd.isna(X.TotalCharges)\n",
    "    #X = X[~drop_these]\n",
    "    #y = y[~drop_these]\n",
    "    \n",
    "    m = OLS(y, X)\n",
    "    o = m.fit()\n",
    "    new_resid = rand_from_Finv(o.resid, rng, size=len(o.resid))\n",
    "    \n",
    "    #df[ycol] = o.predict(X) + new_resid\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_residuals(telco, 'tenure', 'TotalCharges').info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert outlier to influential point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from statsmodels.stats.outliers_influence import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsertOutliers(Stainer):\n",
    "    \"\"\" Stainer to insert outliers at influential points using a linear model. \"\"\"\n",
    "    \n",
    "    def __init__(self, name = \"Inserts outliers\", row_idx =[], col_idx = [], n=5):\n",
    "        \"\"\" Constructor for InsertOutliers\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str, optional.\n",
    "            Name of stainer. \n",
    "        col_idx: int list, required. This should specify at least two columns. The first will\n",
    "                 be used as the y-variable, and the others will be used as the X matrix.\n",
    "        row_idx: int list, unused.\n",
    "        n:       number of outliers to insert. The default is 5.\n",
    "                 \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If col_idx has length not equals to two, or the values are not consecutive.\n",
    "        \"\"\"\n",
    "        if len(col_idx) < 2:\n",
    "            raise ValueError(\"col_idx must contain at least two integers.\")\n",
    "        super().__init__(name, row_idx, col_idx)\n",
    "        self.n = n\n",
    "        \n",
    "    def transform(self, df, rng, row_idx=None, col_idx=None):\n",
    "        \"\"\"Applies staining on the given indices in the provided dataframe.\n",
    "        \n",
    "        A ordinary least squares linear model is fit, using statsmodels. \n",
    "        \n",
    "        The 5 most influential points are identified (using their leverage).\n",
    "        \n",
    "        The residuals for these 5 points are replaced by sampling from the 5% tails of the residual \n",
    "        distributions.\n",
    "        \n",
    "        This stainer should result in a similar fit, but slightly different diagnostics/statistics.\n",
    "        \n",
    "        The user should check if the new y-values are valid or not (e.g. are they negative when they \n",
    "        shouldn't be?)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame \n",
    "            Dataframe to be transformed.\n",
    "        rng : np.random.BitGenerator\n",
    "            PCG64 pseudo-random number generator.\n",
    "        row_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        col_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_df : pd.DataFrame\n",
    "            Modified dataframe, with some columns shifted \"inwards\"\n",
    "        row_map : empty dictionary.\n",
    "        col_map : empty dictionary.\n",
    "       \n",
    "        >>> rng = np.random.default_rng(12)\n",
    "        >>> x = np.arange(1, 11)\n",
    "        >>> x[-2:] = [15,16]\n",
    "\n",
    "        >>> y = x*2 + 3 + rng.normal(scale=5, size=10)\n",
    "        >>> org_df = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "        >>> rr = InsertOutliers('test rr', [], [1,0], n=2)\n",
    "        >>> new_df = rr.transform(org_df, rng)[0]\n",
    "\n",
    "        >>> print(pd.concat((org_df, new_df), axis=1))\n",
    "            x          y   x          y\n",
    "        0   1   4.965866   1   4.965866\n",
    "        1   2  12.230716   2  12.230716\n",
    "        2   3  12.707942   3  12.707942\n",
    "        3   4  14.619783   4  14.619783\n",
    "        4   5  21.093881   5  21.093881\n",
    "        5   6   8.972209   6   8.972209\n",
    "        6   7  13.865223   7  13.865223\n",
    "        7   8  12.396684   8  12.396684\n",
    "        8  15  32.461237  15  39.217787\n",
    "        9  16  39.993818  16  27.541544\n",
    "\n",
    "        \"\"\"\n",
    "        new_df, row_idx, col_idx = self._init_transform(df, row_idx, col_idx)\n",
    "        start = time()\n",
    "        \n",
    "        # drop missing values\n",
    "        col_names = new_df.columns[col_idx]\n",
    "        fit_df = new_df.iloc[:, col_idx].dropna()\n",
    "        \n",
    "        X = add_constant(fit_df.iloc[:, 1:])\n",
    "        y = fit_df.iloc[:,0]\n",
    "    \n",
    "        # obtain a function to sample from residuals\n",
    "        m = OLS(y, X)\n",
    "        o = m.fit()\n",
    "        Finv = rand_from_Finv(o.resid, rng, return_fn=True)\n",
    "        \n",
    "        # derive leverage\n",
    "        hhh4 = pd.Series(np.diag(np.matmul(np.matmul(X.values, \n",
    "                                                     np.linalg.inv(np.matmul(np.transpose(X.values), X.values))), \n",
    "                                           np.transpose(X.values))))\n",
    "        hhh4.index = X.index\n",
    "        replace_these = hhh4.nlargest(self.n).index\n",
    "        \n",
    "        # sample from tails\n",
    "        V = rng.random(size=self.n)\n",
    "        W = [rng.uniform(0.0, 0.05,1)[0] if (x <= 0.5) else rng.uniform(0.95, 1.00, 1)[0] for x in V ]\n",
    "        \n",
    "        # only put new values where we could predict values; otherwise keep old y-values.\n",
    "        new_y = o.predict(X.loc[replace_these,:]) + Finv(W)\n",
    "        #f2 = df.copy(deep=True)\n",
    "        new_df.loc[replace_these, col_names[0]] = new_y        \n",
    "        \n",
    "        end = time()\n",
    "        self.update_history(f\"Outliers in column {col_idx[0]} by sampling from residual distribution.\", \n",
    "                            end-start)\n",
    "        return new_df,{},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "x = np.arange(1, 11)\n",
    "x[-2:] = [15,16]\n",
    "\n",
    "y = x*2 + 3 + rng.normal(scale=5, size=10)\n",
    "org_df = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "#org_df.plot('x', 'y', kind='scatter');\n",
    "\n",
    "rr = InsertOutliers('test rr', [], [1,0], n=2)\n",
    "new_df = rr.transform(org_df, rng)[0]\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "print(pd.concat((org_df, new_df), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.plot('x', 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_outlier(df, ycol, Xcols, n=5):\n",
    "    X = add_constant(df[[Xcols]])\n",
    "    drop_these = pd.isna(X.TotalCharges)\n",
    "    X = X[~drop_these]\n",
    "    hhh4 = pd.Series(np.diag(np.matmul(np.matmul(X.values, \n",
    "                                                 np.linalg.inv(np.matmul(np.transpose(X.values), X.values))), \n",
    "                                       np.transpose(X.values))))\n",
    "    hhh4.index = X.index\n",
    "    replace_these = hhh4.nlargest(n).index\n",
    "    \n",
    "    y = df[ycol]\n",
    "    y = y[~drop_these]\n",
    "    m = OLS(y, X)\n",
    "    o = m.fit()\n",
    "    r1 = o.resid\n",
    "\n",
    "    ecdf1 = ECDF(r1)\n",
    "    Finv = np.hstack((r1.min(), ecdf1.x[1:-1], r1.max()))\n",
    "    U = np.hstack((0.0, ecdf1.y[1:-1], 1.0))\n",
    "    ak2 = Akima1DInterpolator(U, Finv)\n",
    "    \n",
    "    V = np.random.random(n)\n",
    "    W = [np.random.uniform(0.0, 0.05,1)[0] if (x <= 0.5) else np.random.uniform(0.95, 1.00, 1)[0] for x in V ]\n",
    "    \n",
    "    new_y = o.predict(X.loc[replace_these,:]) + ak2(W)\n",
    "    df2 = df.copy(deep=True)\n",
    "    \n",
    "    df2.loc[replace_these, ycol] = new_y\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modify Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr,pearsonr,norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifyCorrelation(Stainer):\n",
    "    \"\"\" Stainer to modify correlation between two columns. \"\"\"\n",
    "    \n",
    "    def __init__(self, name = \"Inserts outliers\", row_idx =[], col_idx = [], rho=None):\n",
    "        \"\"\" Constructor for ModifyCorrelation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str, optional.\n",
    "            Name of stainer. \n",
    "        col_idx: int list, required. This should specify exactly two numeric columns.\n",
    "        row_idx: int list, unused.\n",
    "        rho:     New correlation between the two columns.\n",
    "                 \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If col_idx has length not equals to two, or the values are not consecutive.\n",
    "        \"\"\"\n",
    "        if len(col_idx) != 2:\n",
    "            raise ValueError(\"col_idx must contain exactly two integers.\")\n",
    "        if rho is None:\n",
    "            raise ValueError(\"rho needs to specified.\")\n",
    "        super().__init__(name, row_idx, col_idx)\n",
    "        self.rho = rho\n",
    "        \n",
    "    def transform(self, df, rng, row_idx=None, col_idx=None):\n",
    "        \"\"\"Applies staining on the given indices in the provided dataframe.\n",
    "        \n",
    "        A multivariate normal copula is used, with the Finv being fitted using the Akima interpolator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame \n",
    "            Dataframe to be transformed.\n",
    "        rng : np.random.BitGenerator\n",
    "            PCG64 pseudo-random number generator.\n",
    "        row_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        col_idx : int list, optional\n",
    "            Unused parameter.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_df : pd.DataFrame\n",
    "        row_map : empty dictionary.\n",
    "        col_map : empty dictionary.\n",
    "       \n",
    "        >>> rng = np.random.default_rng(12)\n",
    "        >>> x = np.arange(0, 100)\n",
    "        >>> y = x*2 + 3 + rng.normal(scale=25, size=100)\n",
    "        >>> org_df = pd.DataFrame({'x':x, 'y':y})\n",
    "        >>> spearmanr(org_df.x, org_df.y)[0]\n",
    "        0.9324692469246924\n",
    "\n",
    "        >>> rr = ModifyCorrelation('test rr', [], [1,0], rho=0.0)\n",
    "        >>> new_df = rr.transform(org_df, rng)[0]\n",
    "        >>> spearmanr(new_df.x, new_df.y)[0]\n",
    "        0.12897689768976897\n",
    "        \"\"\"\n",
    "        new_df, row_idx, col_idx = self._init_transform(df, row_idx, col_idx)\n",
    "        start = time()\n",
    "\n",
    "        # drop missing values\n",
    "        col_names = new_df.columns[col_idx]\n",
    "        fit_df = new_df.iloc[:, col_idx].dropna()\n",
    "        \n",
    "        # estimate Finv, Ginv\n",
    "        X1 = fit_df.iloc[:, 0]\n",
    "        Finv = rand_from_Finv(X1, rng, return_fn=True)\n",
    "        X2 = fit_df.iloc[:, 1]\n",
    "        Ginv = rand_from_Finv(X2, rng, return_fn=True)\n",
    "        \n",
    "        org_corr = spearmanr(X1, X2)[0]\n",
    "\n",
    "        # sample multivariate normal with desired correlation\n",
    "        mu = np.zeros(2)\n",
    "        sigma = np.ones((2,2))\n",
    "        sigma[0,1] = sigma[1,0] = self.rho\n",
    "        Xn = rng.multivariate_normal(mu, sigma, size=new_df.shape[0], method='cholesky')\n",
    "\n",
    "        # apply norm cdf to array\n",
    "        Yn = norm.cdf(Xn)\n",
    "\n",
    "        # apply Finv, Ginv\n",
    "        new_col1 = Finv(Yn[:,0])\n",
    "        new_col2 = Ginv(Yn[:,1])\n",
    "        new_df.loc[:, col_names[0]] = new_col1\n",
    "        new_df.loc[:, col_names[1]] = new_col2\n",
    "        \n",
    "        end = time()\n",
    "        self.update_history(f\"Correlation modified from {org_corr:.2f} to {self.rho:.2f}.\", end-start)\n",
    "        return new_df,{},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = ModifyCorrelation('test1', col_idx=[5,19], rho=-0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = mm.transform(telco, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(tmp.tenure, tmp.TotalCharges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12)\n",
    "x = np.arange(0, 100)\n",
    "y = x*2 + 3 + rng.normal(scale=25, size=100)\n",
    "org_df = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "spearmanr(org_df.x, org_df.y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_df.plot('x', 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#org_df.plot('x', 'y', kind='scatter');\n",
    "\n",
    "rr = ModifyCorrelation('test rr', [], [1,0], rho=0.0)\n",
    "new_df = rr.transform(org_df, rng)[0]\n",
    "\n",
    "#print(pd.concat((org_df, new_df), axis=1))\n",
    "\n",
    "spearmanr(new_df.x, new_df.y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.plot('x', 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_correlation(df, col1, col2, rho):\n",
    "    # estimate/interpolate cdf for col1 and col2\n",
    "    ecdf1 = ECDF(df[col1])\n",
    "    Finv_step = np.hstack((df[col1].min(), ecdf1.x[1:-1], df[col1].max()))\n",
    "    U = np.hstack((0.0, ecdf1.y[1:-1], 1.0))\n",
    "    Finv = Akima1DInterpolator(U, Finv_step)\n",
    "\n",
    "    ecdf2 = ECDF(df[col2])\n",
    "    Ginv_step = np.hstack((df[col2].min(), ecdf2.x[1:-1], df[col2].max()))\n",
    "    U = np.hstack((0.0, ecdf2.y[1:-1], 1.0))\n",
    "    Ginv = Akima1DInterpolator(U, Ginv_step)\n",
    "    \n",
    "    # print original correlation\n",
    "    org_corr = spearmanr(df[col1], df[col2])\n",
    "    #print(f'The original spearman correlation is {org_corr[0]:.2f}.')\n",
    "    \n",
    "    # sample multivariate normal with desired correlation\n",
    "    rng = default_rng()\n",
    "    mu = np.zeros(2)\n",
    "    sigma = np.ones((2,2))\n",
    "    sigma[0,1] = sigma[1,0] = rho\n",
    "    Xn = rng.multivariate_normal(mu, sigma, size=df.shape[0], method='cholesky')\n",
    "    \n",
    "    # apply norm cdf to array\n",
    "    Yn = norm.cdf(Xn)\n",
    "    \n",
    "    # apply Finv, Ginv\n",
    "    new_col1 = Finv(Yn[:,0])\n",
    "    new_col2 = Ginv(Yn[:,1])\n",
    "    \n",
    "    df2 = df.copy(deep=True)\n",
    "    df2[col1] = new_col1\n",
    "    df2[col2] = new_col2\n",
    "    \n",
    "    # return df\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco.tenure.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = telco[['TotalCharges']]\n",
    "y = telco.tenure\n",
    "X = add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhh4 = pd.Series(np.diag(np.matmul(np.matmul(X.values, np.linalg.inv(np.matmul(np.transpose(X.values), X.values))), np.transpose(X.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhh4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhh.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these = pd.isna(X.TotalCharges)\n",
    "X = X[~drop_these]\n",
    "y = y[~drop_these]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = OLS(y, X)\n",
    "\n",
    "o = m.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y - (o.predict(X) + o.resid)).sum()\n",
    "\n",
    "rand_from_Finv(o.resid, size=len(o.resid)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out['MultipleLines'] + out['OnlineBackup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[['Description', 'InvoiceNo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb.runcall(column_splitter, d2, 'Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.iloc[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.combine_first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.Series([True, False, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id.index = qty_series.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_series[id] = d2.Description[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2a = d2.Description.str.split('-',n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2a[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2[[\"Quantity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.combine_first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([d2a[[1]],d2[[\"Quantity\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo.Description.str.contains(\"[^\\w\\s]\", regex=True).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo2 = oo[~oo.Description.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = oo2[oo2.Description.str.contains(\"[^\\w\\s]\", regex=True)].Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.str.split(\"(?=-)\",n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
